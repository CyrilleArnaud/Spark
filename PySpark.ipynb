{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c6fc9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install findspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0f338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "159083dc",
   "metadata": {},
   "source": [
    "## Le traitement et l'analyse de données avec PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0c76af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                        ## L'importation des bibliothèques ##\n",
    "    \n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b8713f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                            ## La création d'une session ##\n",
    "    \n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "92ad605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## L'affichage du premier message ##\n",
    "    \n",
    "df = spark.sql(''' select  'spark' as hello ''')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a514ada5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-81S4HPT:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1be522e2520>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8ab17c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "        ## L'importation de notre jeu de données. C'est la meilleur façon d'importer la base de données dans pyspark. ##\n",
    "\n",
    "df_pyspark = spark.read.csv(\"C:/Users/dell/Desktop/Master_2  Econométrie/Big_Data/ML_pipeline/ML_pipeline/test2.csv\",\n",
    "                            header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3c1f935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                    ## L'affichage du jeu de données ##\n",
    "    \n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "36eee39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                        ## La deuxième version d'importation d'un fichier ##\n",
    "    \n",
    "df_pyspark1 = spark.read.option('header', 'true').csv('C:/Users/dell'+\n",
    "                                                      '/Desktop/Master_2  Econométrie/Big_Data/ML_pipeline/ML_pipeline/test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eb604f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## L'affichage du jeu de données ##\n",
    "    \n",
    "df_pyspark1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d365b6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                             ## La nature des colonnes de notre jeu de données et leurs noms. ##\n",
    "df_pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2ad05e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Name: string, age: int, Experience: int, Salary: int]>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                             ## La nature des différentes colonnes. ##\n",
    "    \n",
    "df_pyspark.printSchema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "32787df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3bb46b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                             ## Le type de l'objet de notre analyse ##\n",
    "    \n",
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6b426dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', age=31, Experience=10, Salary=30000),\n",
       " Row(Name='Sudhanshu', age=30, Experience=8, Salary=25000),\n",
       " Row(Name='Sunny', age=29, Experience=4, Salary=20000)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                     ## Les trois premières lignes de notre jeu de données ##\n",
    "    \n",
    "df_pyspark.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "71672859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                        ## Les différentes colonnes de mon jeu de données ##\n",
    "    \n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d0458",
   "metadata": {},
   "source": [
    "## ** Step 0:  La statistique descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e911420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+------------------+-----------------+\n",
      "|summary|  Name|               age|        Experience|           Salary|\n",
      "+-------+------+------------------+------------------+-----------------+\n",
      "|  count|     7|                 8|                 7|                8|\n",
      "|   mean|  null|              28.5| 5.428571428571429|          25750.0|\n",
      "| stddev|  null|5.3718844791323335|3.8234863173611093|9361.776388210581|\n",
      "|    min|Harsha|                21|                 1|            15000|\n",
      "|    max| Sunny|                36|                10|            40000|\n",
      "+-------+------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                    ## Les statistiques usuelles de chaque variable ##\n",
    "    \n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191f575",
   "metadata": {},
   "source": [
    "## ** Step 1:  La manipulation de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a60cdaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                ## La nature d'une variable en fonction de son nom ##\n",
    "    \n",
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bc5000fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    Krish|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "|     Paul|\n",
      "|   Harsha|\n",
      "|  Shubham|\n",
      "|   Mahesh|\n",
      "|     null|\n",
      "|     null|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                 ## La selection d'une seule variable, notamment Name. ##\n",
    "    \n",
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3974365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|     null|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "|    Krish|\n",
      "|   Harsha|\n",
      "|     Paul|\n",
      "|  Shubham|\n",
      "|   Mahesh|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                        ## La selection des modalités d'une seule variable, notamment Name. ##\n",
    "    \n",
    "df_pyspark.select(['Name']).distinct().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8c5b5f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|     Name| age|\n",
      "+---------+----+\n",
      "|    Krish|  31|\n",
      "|Sudhanshu|  30|\n",
      "|    Sunny|  29|\n",
      "|     Paul|  24|\n",
      "|   Harsha|  21|\n",
      "|  Shubham|  23|\n",
      "|   Mahesh|null|\n",
      "|     null|  34|\n",
      "|     null|  36|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                 ## La selection deux variables, notamment Name et 'age'. ##\n",
    "    \n",
    "df_pyspark.select(['Name', 'age']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31cfed3",
   "metadata": {},
   "source": [
    "## 1 - Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "caade6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Experience: int, Salary: int, Age_2: int]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                            ## Add the columns ##\n",
    "    \n",
    "df_pyspark_2 = df_pyspark.withColumn('Age_2', df_pyspark['age']+2)\n",
    "df_pyspark_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "34f88448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----+\n",
      "|     Name| age|Experience|Salary|Age_2|\n",
      "+---------+----+----------+------+-----+\n",
      "|    Krish|  31|        10| 30000|   33|\n",
      "|Sudhanshu|  30|         8| 25000|   32|\n",
      "|    Sunny|  29|         4| 20000|   31|\n",
      "|     Paul|  24|         3| 20000|   26|\n",
      "|   Harsha|  21|         1| 15000|   23|\n",
      "|  Shubham|  23|         2| 18000|   25|\n",
      "|   Mahesh|null|      null| 40000| null|\n",
      "|     null|  34|        10| 38000|   36|\n",
      "|     null|  36|      null|  null|   38|\n",
      "+---------+----+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ab721",
   "metadata": {},
   "source": [
    "## 2 - Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3c92c8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_3 = df_pyspark_2.drop('Age_2')\n",
    "df_pyspark_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7707fb7c",
   "metadata": {},
   "source": [
    "## 3 - Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "03f01d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "| New_Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_4 = df_pyspark_3.withColumnRenamed('Name', 'New_Name')\n",
    "df_pyspark_4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5a36e",
   "metadata": {},
   "source": [
    "## 4 -  Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b663c532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  31|        10| 30000|\n",
      "|  30|         8| 25000|\n",
      "|  29|         4| 20000|\n",
      "|  24|         3| 20000|\n",
      "|  21|         1| 15000|\n",
      "|  23|         2| 18000|\n",
      "|null|      null| 40000|\n",
      "|  34|        10| 38000|\n",
      "|  36|      null|  null|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_2p_1 = df_pyspark.drop('Name')\n",
    "df_pyspark_2p_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f0f5d37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                        ## La suppression des NAs ##\n",
    "    \n",
    "df_pyspark_2p_2 = df_pyspark.na.drop()\n",
    "df_pyspark_2p_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "004e4f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                    ## Avec all, on conserve toutes les valeurs nulles. ##\n",
    "    \n",
    "df_pyspark_2p_3 = df_pyspark.na.drop(how='all')\n",
    "df_pyspark_2p_3.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a7f77d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                        ## Avec all, on supprime toutes les valeurs nulles comme df_pyspark.na.drop() ##\n",
    "    \n",
    "df_pyspark_2p_4 = df_pyspark.na.drop(how='any')\n",
    "df_pyspark_2p_4.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "99e5e858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "        ## Avec all, on supprime toutes les valeurs nulles comme df_pyspark.na.drop(). Mais il ne faut pas supprimer ##\n",
    "                               ## les lignes ayant deux ou plus de deux valeurs non nulles ##\n",
    "\n",
    "df_pyspark_2p_5 = df_pyspark.na.drop(how='any', thresh = 2)\n",
    "df_pyspark_2p_5.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "714f917a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     null| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                #dropna() with subset : ## Il faut supprimer les nas en fonction de la variable 'Expérience'#\n",
    "    \n",
    "df_pyspark_2p_6 = df_pyspark.na.drop(how='any', subset = ['Experience'])\n",
    "df_pyspark_2p_6.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c8ca0",
   "metadata": {},
   "source": [
    "## 5 - Filling the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7e509969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         Krish|  31|        10| 30000|\n",
      "|     Sudhanshu|  30|         8| 25000|\n",
      "|         Sunny|  29|         4| 20000|\n",
      "|          Paul|  24|         3| 20000|\n",
      "|        Harsha|  21|         1| 15000|\n",
      "|       Shubham|  23|         2| 18000|\n",
      "|        Mahesh|null|      null| 40000|\n",
      "|Missing Values|  34|        10| 38000|\n",
      "|Missing Values|  36|      null|  null|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                ## Le remplacement des valeurs manquantes par une 'Missing values' ##\n",
    "    \n",
    "df_pyspark_2p_7 = df_pyspark.na.fill('Missing Values')\n",
    "df_pyspark_2p_7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0bc5a640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh|  0|         0| 40000|\n",
      "|     null| 34|        10| 38000|\n",
      "|     null| 36|         0|     0|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                    ## Le remplacement par 0 des valeurs nulles ##\n",
    "    \n",
    "df_pyspark_2p_8 = df_pyspark.na.fill(0, ['Experience', 'Salary', 'age'])\n",
    "df_pyspark_2p_8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c3a6b",
   "metadata": {},
   "source": [
    "## 6 - Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "14e42503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c32df6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "                    inputCols = ['age', 'Experience', 'Salary'],\n",
    "                    outputCols = [\"{}.imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    "                ).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d5cdfe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age.imputed|Experience.imputed|Salary.imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|null|      null| 40000|         28|                 5|         40000|\n",
      "|     null|  34|        10| 38000|         34|                10|         38000|\n",
      "|     null|  36|      null|  null|         36|                 5|         25750|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                ## Add imputation on the cols: par la moyenne ##\n",
    "    \n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "300b4dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_2p_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ae659",
   "metadata": {},
   "source": [
    "## 7 - Filter opérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "87cc9c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                        ## Salary of the people less than 20000 ##\n",
    "    \n",
    "df_pyspark_2p_2.filter(\"Salary>=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ac425e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                        ## Salary of the people less than 20000 ##\n",
    "    \n",
    "df_pyspark_2p_2.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "592ae168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   Name|\n",
      "+---+-------+\n",
      "| 29|  Sunny|\n",
      "| 24|   Paul|\n",
      "| 21| Harsha|\n",
      "| 23|Shubham|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                    ## Salary of the people less than 20000 avec la selection des variables #age et #Name ##\n",
    "    \n",
    "df_pyspark_2p_2.filter(\"Salary<=20000\").select([\"age\", \"Name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6b822864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Sunny| 29|         4| 20000|\n",
      "| Paul| 24|         3| 20000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## Normal conditions with ##\n",
    "    \n",
    "df_pyspark_2p_2.filter(df_pyspark_2p_2[\"Salary\"]==20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "045edaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## pas Normal conditions with ##\n",
    "    \n",
    "df_pyspark_2p_2.filter(~(df_pyspark_2p_2[\"Salary\"]==20000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ad72ca77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## pas Normal conditions with ##\n",
    "    \n",
    "df_pyspark.filter((df_pyspark[\"Salary\"]<=20000) & (df_pyspark[\"age\"]>=31)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f045c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     null| 34|        10| 38000|\n",
      "|     null| 36|      null|  null|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## pas Normal conditions with ##\n",
    "    \n",
    "df_pyspark.filter((df_pyspark[\"Salary\"]<=20000) | (df_pyspark[\"age\"]>=25)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "035c6715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                        ## Multiple conditions with and ##\n",
    "    \n",
    "df_pyspark_2p_2.filter((df_pyspark_2p_2[\"Salary\"]<=20000) & (df_pyspark_2p_2[\"Salary\"]>=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "44b5d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                        ## Multiple conditions with or ##\n",
    "    \n",
    "df_pyspark_2p_2.filter((df_pyspark_2p_2[\"Salary\"]<=20000) | (df_pyspark_2p_2[\"Salary\"]>=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "60919998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## Unique condition not ##\n",
    "    \n",
    "df_pyspark_2p_2.filter(~(df_pyspark_2p_2[\"Salary\"]<=20000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f9acf",
   "metadata": {},
   "source": [
    "## 8-  Pyspark Groupby Operations and Aggregrate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e70b24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                    ## L'importation de la base de données ##\n",
    "    \n",
    "df_pysparkG = spark.read.csv(\"C:/Users/dell/Desktop/Master_2  Econométrie/Big_Data/ML_pipeline/ML_pipeline/test3.csv\",\n",
    "                             header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "44382a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pysparkG.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "97220b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pysparkG.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2b3ba5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## Le salaire par département ##\n",
    "    \n",
    "df_pysparkG.select(['Departments','salary']).groupBy(['Departments']).sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b02c270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|max(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      10000|\n",
      "|    Big Data|       5000|\n",
      "|Data Science|      20000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## Le salaire max par département ##\n",
    "\n",
    "df_pysparkG.select(['Departments','salary']).groupBy(['Departments']).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "08604b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|min(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|       5000|\n",
      "|    Big Data|       2000|\n",
      "|Data Science|       3000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## Le salaire min par département ##\n",
    "    \n",
    "df_pysparkG.select(['Departments','salary']).groupBy(['Departments']).min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "057f37cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## La moyenne par département ##\n",
    "\n",
    "df_pysparkG.select(['Departments','salary']).groupBy(['Departments']).avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "40b17afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                         ## Le nombre de personnes par département ##\n",
    "    \n",
    "df_pysparkG.select(['Departments','salary']).groupBy(['Departments']).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e6108c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------+\n",
      "| Departments|     Name|salary|\n",
      "+------------+---------+------+\n",
      "|Data Science|    Krish| 10000|\n",
      "|         IOT|    Krish|  5000|\n",
      "|    Big Data|   Mahesh|  4000|\n",
      "|    Big Data|    Krish|  4000|\n",
      "|Data Science|   Mahesh|  3000|\n",
      "|Data Science|Sudhanshu| 20000|\n",
      "|         IOT|Sudhanshu| 10000|\n",
      "|    Big Data|Sudhanshu|  5000|\n",
      "|Data Science|    Sunny| 10000|\n",
      "|    Big Data|    Sunny|  2000|\n",
      "+------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                                ## Ranger par ordre croissant ##\n",
    "    \n",
    "df_pysparkG.select(sorted(df_pysparkG.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e6c1429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La moyenne de salaires par département: C'est bien\n",
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## Groupby ## Grouped the maximum salary ##\n",
    "    \n",
    "print('La moyenne de salaires par département: ' + 'C\\'est bien')\n",
    "df_pysparkG.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02972134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                        ## Groupby ## Brouped the maximum salary ##\n",
    "    \n",
    "df_pysparkG.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "64467d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|max(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      20000|\n",
      "|    Sunny|      10000|\n",
      "|    Krish|      10000|\n",
      "|   Mahesh|       4000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## Groupby ## Brouped the maximum salary ##\n",
    "    \n",
    "df_pysparkG.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cadc5269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|min(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|       5000|\n",
      "|    Sunny|       2000|\n",
      "|    Krish|       4000|\n",
      "|   Mahesh|       3000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## Groupby ## Brouped the minimum salary ##\n",
    "\n",
    "df_pysparkG.groupBy('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e47e3395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|       avg(salary)|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## Groupby ## Brouped the moyenne salary ##\n",
    "\n",
    "df_pysparkG.groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "891cd40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                                ## Groupby departement salary ##\n",
    "    \n",
    "df_pysparkG.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5875ac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                            ## Groupby departement salary : la moyenne ##\n",
    "    \n",
    "df_pysparkG.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bedf1f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                    ## Groupby departement salary : la comptage ##\n",
    "    \n",
    "df_pysparkG.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0308c736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                                ## La somme totale des salaire ##\n",
    "\n",
    "df_pysparkG.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef30e756",
   "metadata": {},
   "source": [
    "## 9 - Filter opérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b74e4cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                ## Elle permet d'utiliser les nombreuses fonctions de SQL, notamment #agg ##\n",
    "    \n",
    "import pyspark.sql.functions as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cf9cd2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_pysparkG.groupBy(['Departments']).agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dd7bd767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+-----------+\n",
      "|     Name| Departments|salary|     classe|\n",
      "+---------+------------+------+-----------+\n",
      "|    Krish|Data Science| 10000|Econométrie|\n",
      "|    Krish|         IOT|  5000|Econométrie|\n",
      "|   Mahesh|    Big Data|  4000|Econométrie|\n",
      "|    Krish|    Big Data|  4000|Econométrie|\n",
      "|   Mahesh|Data Science|  3000|Econométrie|\n",
      "|Sudhanshu|Data Science| 20000|Econométrie|\n",
      "|Sudhanshu|         IOT| 10000|Econométrie|\n",
      "|Sudhanshu|    Big Data|  5000|Econométrie|\n",
      "|    Sunny|Data Science| 10000|Econométrie|\n",
      "|    Sunny|    Big Data|  2000|Econométrie|\n",
      "+---------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pysparkG.withColumn('classe', F.lit('Econométrie')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "91a9bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+------+\n",
      "|     Name| Departments|salary|classe|\n",
      "+---------+------------+------+------+\n",
      "|    Krish|Data Science| 10000| 20000|\n",
      "|    Krish|         IOT|  5000| 20000|\n",
      "|   Mahesh|    Big Data|  4000| 20000|\n",
      "|    Krish|    Big Data|  4000| 20000|\n",
      "|   Mahesh|Data Science|  3000| 20000|\n",
      "|Sudhanshu|Data Science| 20000| 20000|\n",
      "|Sudhanshu|         IOT| 10000| 20000|\n",
      "|Sudhanshu|    Big Data|  5000| 20000|\n",
      "|    Sunny|Data Science| 10000| 20000|\n",
      "|    Sunny|    Big Data|  2000| 20000|\n",
      "+---------+------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pysparkG.withColumn('classe', F.lit(df_pysparkG.agg(F.max('Salary')).collect()[0][0])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1af49df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d71600eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6e0692f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pysparkG.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26f3a2",
   "metadata": {},
   "source": [
    "## 10- Les jointures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d4637d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+---------+----+----------+------+\n",
      "|     Name| Departments|salary|     Name| age|Experience|Salary|\n",
      "+---------+------------+------+---------+----+----------+------+\n",
      "|    Krish|Data Science| 10000|    Krish|  31|        10| 30000|\n",
      "|    Krish|         IOT|  5000|    Krish|  31|        10| 30000|\n",
      "|   Mahesh|    Big Data|  4000|   Mahesh|null|      null| 40000|\n",
      "|    Krish|    Big Data|  4000|    Krish|  31|        10| 30000|\n",
      "|   Mahesh|Data Science|  3000|   Mahesh|null|      null| 40000|\n",
      "|Sudhanshu|Data Science| 20000|Sudhanshu|  30|         8| 25000|\n",
      "|Sudhanshu|         IOT| 10000|Sudhanshu|  30|         8| 25000|\n",
      "|Sudhanshu|    Big Data|  5000|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|Data Science| 10000|    Sunny|  29|         4| 20000|\n",
      "|    Sunny|    Big Data|  2000|    Sunny|  29|         4| 20000|\n",
      "+---------+------------+------+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                    ##inner join ##\n",
    "df_pysparkG.join(df_pyspark1,\n",
    "               df_pysparkG.Name == df_pyspark1.Name,\n",
    "               \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0dd2c4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+---------+----+----------+------+\n",
      "|     Name| Departments|salary|     Name| age|Experience|Salary|\n",
      "+---------+------------+------+---------+----+----------+------+\n",
      "|     null|        null|  null|     null|  34|        10| 38000|\n",
      "|     null|        null|  null|     null|  36|      null|  null|\n",
      "|Sudhanshu|Data Science| 20000|Sudhanshu|  30|         8| 25000|\n",
      "|Sudhanshu|         IOT| 10000|Sudhanshu|  30|         8| 25000|\n",
      "|Sudhanshu|    Big Data|  5000|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|Data Science| 10000|    Sunny|  29|         4| 20000|\n",
      "|    Sunny|    Big Data|  2000|    Sunny|  29|         4| 20000|\n",
      "|    Krish|Data Science| 10000|    Krish|  31|        10| 30000|\n",
      "|    Krish|         IOT|  5000|    Krish|  31|        10| 30000|\n",
      "|    Krish|    Big Data|  4000|    Krish|  31|        10| 30000|\n",
      "|     null|        null|  null|   Harsha|  21|         1| 15000|\n",
      "|     null|        null|  null|     Paul|  24|         3| 20000|\n",
      "|     null|        null|  null|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|    Big Data|  4000|   Mahesh|null|      null| 40000|\n",
      "|   Mahesh|Data Science|  3000|   Mahesh|null|      null| 40000|\n",
      "+---------+------------+------+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                                ## Full Join ##\n",
    "df_pysparkG.join(df_pyspark1,\n",
    "               df_pysparkG.Name == df_pyspark1.Name,\n",
    "               \"full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1730866d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+---------+----+----------+------+\n",
      "|     Name| Departments|salary|     Name| age|Experience|Salary|\n",
      "+---------+------------+------+---------+----+----------+------+\n",
      "|    Krish|Data Science| 10000|    Krish|  31|        10| 30000|\n",
      "|    Krish|         IOT|  5000|    Krish|  31|        10| 30000|\n",
      "|   Mahesh|    Big Data|  4000|   Mahesh|null|      null| 40000|\n",
      "|    Krish|    Big Data|  4000|    Krish|  31|        10| 30000|\n",
      "|   Mahesh|Data Science|  3000|   Mahesh|null|      null| 40000|\n",
      "|Sudhanshu|Data Science| 20000|Sudhanshu|  30|         8| 25000|\n",
      "|Sudhanshu|         IOT| 10000|Sudhanshu|  30|         8| 25000|\n",
      "|Sudhanshu|    Big Data|  5000|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|Data Science| 10000|    Sunny|  29|         4| 20000|\n",
      "|    Sunny|    Big Data|  2000|    Sunny|  29|         4| 20000|\n",
      "+---------+------------+------+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                                ## left Join ##\n",
    "df_pysparkG.join(df_pyspark1,\n",
    "               df_pysparkG.Name == df_pyspark1.Name,\n",
    "               \"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d3d36820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+---------+----+----------+------+\n",
      "|     Name| Departments|salary|     Name| age|Experience|Salary|\n",
      "+---------+------------+------+---------+----+----------+------+\n",
      "|    Krish|    Big Data|  4000|    Krish|  31|        10| 30000|\n",
      "|    Krish|         IOT|  5000|    Krish|  31|        10| 30000|\n",
      "|    Krish|Data Science| 10000|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|    Big Data|  5000|Sudhanshu|  30|         8| 25000|\n",
      "|Sudhanshu|         IOT| 10000|Sudhanshu|  30|         8| 25000|\n",
      "|Sudhanshu|Data Science| 20000|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|    Big Data|  2000|    Sunny|  29|         4| 20000|\n",
      "|    Sunny|Data Science| 10000|    Sunny|  29|         4| 20000|\n",
      "|     null|        null|  null|     Paul|  24|         3| 20000|\n",
      "|     null|        null|  null|   Harsha|  21|         1| 15000|\n",
      "|     null|        null|  null|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|Data Science|  3000|   Mahesh|null|      null| 40000|\n",
      "|   Mahesh|    Big Data|  4000|   Mahesh|null|      null| 40000|\n",
      "|     null|        null|  null|     null|  34|        10| 38000|\n",
      "|     null|        null|  null|     null|  36|      null|  null|\n",
      "+---------+------------+------+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                                ## Right Join ##\n",
    "df_pysparkG.join(df_pyspark1,\n",
    "               df_pysparkG.Name == df_pyspark1.Name,\n",
    "               \"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27937dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
